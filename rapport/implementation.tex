
\section{Verification method}
\label{alg1}
As a result of lemma \ref{lemma1}, if a system is found to be safe with regard to \e{Bad} for any buffer size \e{k}, it will also be safe for any buffer size larger than \e{k}. Verifying the correctness of a system can be done with the following algorithm.

\begin{algorithm}
  \caption{Verification algorithm}\label{euclid}
  \begin{algorithmic}[1]
      \For{\texttt{$r\not=0$}}
        \If {$\mathcal{R}_k$ $\cap$ $Bad$ $\neq$ $\emptyset$} 
        \State return Unsafe
        \EndIf
        \State V := $\mu X.\alpha_k(I)$ $\cup$ $Apost_k(X)$
        \If {$\gamma_k(V)$ $\cap$ $Bad$ = $\emptyset$} 
        \State return Safe     
        \EndIf
      \EndFor
\end{algorithmic}
\end{algorithm}

The algorithm has two main parts; performing a reachability analysis in order to compute $\mathcal{R}_k$ and check for bad states, and computing an overapproximation of configurations of size $k$, reachable through configurations of size at most $k+1$ and checking for bad states. For a buffer size \e{k}, If a bad state is found to be reachable in $\mathcal{R}_k$, the system is unsafe and the algorithm terminates. If no bad configuration could be reached, $Apost_k$ is applied iteratively until a fixpoint \e{V} is reached. At this point, if the set \e{V} is safe, the system can be said to be safe and the algorithm terminates. If on the other hand a bad state was found, the system is not necessarily unsafe, as \e{V} is an over-approximation of the reachable states in $\mathcal{R}_k$, the process is repeated with a buffer size of \e{k+1}.

In this rest of this section, we show how to implement this algorithm in an efficient way. We do this by beginning with a naive algorithm computing the configurations, and then step by step locating and addressing the performance issues. 

\subsection{Naive algorithm}
A naive way to generate configurations would be to perform the described operations in a way corresponding directly to the mathematical description. Here the configurations are stored in a set datastructure, thus each configuration appears at most once in the set. For each element \e{c} of the set \e{V}, the function \e{gamma}, \e{step} and \e{alpha} are performed. We assume that a set of channel symbols and a set of rules \e{R} describing the transitions are known. A rule \e{r} $\in$ \e{R} is a set of \e{predicates} - if all predicates are true for a \e{c} the rule results in a post-value \e{c'}, otherwise it results in an \e{null}-configuration.

\begin{enumerate}
\item
Gamma: For all configurations \e{c} $\in$ \e{V}, generate all configurations with channel evaluation of larger size than \e{c}. For any such configuration \e{c'}, remove those for which $alpha_k(c')$ $\not\subset$ $V$. This results the set of concretizations \e{C} of \e{c}.

\item
step: For every \e{con} $\in$ \e{C}, \e{r} $\in$ \e{R}, compute \e{r(con)}. This results in the post-image \e{post} of \e{C}.

\item
Alpha: For each element \e{p} $\in$ \e{post}, compute its views of size \e{k} and add them to \e{V'}. If \e{V'} $\cup$ \e{V} = \e{V}, a fixpoint is reached, otherwise repeat the processes with \e{V} := \e{V} $\cup$ \e{V'}.
\end{enumerate}

Although correct, this method is far from efficient. One of the main reasons is that operations are being repeated. Since if in any iteration a concretization is found, that concretization is a valid concretization in all following iterations and \e{step} and \e{alpha} will be applied again without resulting in any configurations not previously observed.

The function \e{gamma} is the heaviest of the functions. It first creates all potential concretizations of a configuration \e{c}, i.e. all combinations of channel evaluations where at least one of the channels is of a larger size than in \e{c}. Additionaly, for each of these, all their views must be computed in order to determine whether the concretization should be refuted or not.

\subsection{Less naive algorithm}
We look closer at $\gamma$, in order to address the issues mentioned above. We will show that all concretizations can be found, while considering only a subset of the potential concretizations. Furthermore, it is possible to determine whether a concretization should be refuted or not, while computing only a subset of its views. 


\subsection{Finding Minimal Traces}
When running the tool, if a bad state is found we want to produce a trace leading up to the bad state. Preferably, this would be a minimal trace that leads to the bad state.
 
The proposed verification method generates a finite set of reachable states (nodes in this context), but it does not record the available transitions between the nodes (i.e. the edges). It is possible to for each node \e{n} to save all nodes \e{n} from which an edge to \e{n'} exists, and thus build the complete reachability graph of the problem. There exists efficient algorithms to solve such a problem, e.g. \e{dijkstra shortest path}, or even the shortest path between any two nodes, e.g. flow-network techniques. Although these algorithms are efficient, building the complete reachability graph would be costly in terms of memory space, as the number of edges may be much larger than the number of states.

 
We show that due to the method of iteratively constructing the graph, nodes are created in such a way, that if a node $n_{i}$ created in the \e{i}:th iteration is reached by a node $n_{i-1}$ over an edge $e_{i-1}$, the shortest path from the initial node $n_0$ will necessarily be a path $e_1...e_{i-1}$.
 
\e{Proof.} This is proven using an induction proof. We hypothesize that, if at the point of creation of $n_i$, choosing the parent node $n_{i-1}$ from which an edge $e_{i-1}$ can be taken $n_i$, the path $e_1..e_{i}$ will be the shortest path to $n_i$ and has length \e{i}. Note that the node $n_{i-1}$ must have been created in the previous iteration; had it been created earlier, the edge $e_i$ could have been taken in a previous iteration, and so $n_{i+1}$ would already be a node in the tree.
 
The base case is that for any node reachable from $n_0$ over any edge $e_0$, $e_0$ will be the shortest path and has length 1. This is trivially true.
 
Now suppose a node $n_{i+1}$ is reachable over an edge $e_i$ from a node $n_i$, and the node $n_{i+1}$ is not yet in the system. The induction hypothesis states that the path $e_1...e_i$ is the shortest path leading up to $n_i$. If $e_0..e_{i-1}e_i$ would not be the shortest path to $n_{i+1}$, there would be a path $e'_0..e'_{k-1}$ to another node $n_k$ with k < i from which $n_{i+1}$ can be reached. But any such node would have been created in the \e{k}:th iteration of the algorithm, which would contradict the fact that the node $n_{i+1}$ was not already in the system.
 
Having shown this, we need only record the information of a single parent of a node, in order to build up a tree from which the shortest path from $n_0$ to any node in the system can efficiently be found.

\swreceiver

\swobserver

\swsender
