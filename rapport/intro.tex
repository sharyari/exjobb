\newpage
\section{Definitions and Terms}

\subsection{Channel Systems}
We present here the basic definition of a finite-state system with unbounded channels. Such a system can be seen as to have two parts, a \e{control part} and a \e{channel part}. The channel part is a set of channels, which may be empty or contain a sequence of messages, a \e{word}. The control part is a labeled finite-state transition system\todo{Should I make labels explicit in the definition below?} The definition below is associated to systems operating on FIFO buffers, other types of systems are covered in section \ref{extensions}. 

\paragraph{Alternating Bit Protocol.} The Alternating Bit Protocol (ABP) is a distributed protocol for transmitting data from a \e{sender} to a \e{receiver} in a network. The protocol uses two unbounded channels, $ch_M$ used to transmit messages and $ch_A$ to transmit \e{acknowledgements} of received messages. The sender sends a message with sequence number x $\in$ \{0,1\} to the receiver over channel $ch_M$, who upon reception sends an acknowledgement with the same sequence number over channel $ch_A$. Both the sender and the receiver may send the same message (with the same sequence number) repeatedly. When the sender receives an acknowledgement from the receiver, the next message can be sent using the sequence number (1-x) (thus the name Alternating Bit Protocol). This is a simple protocol that operates on unbounded channels, and it is used in this paper to illustrate the theoretical concepts in a concrete setting. The behaviour of the sender and receiver process is illustrated in figure \ref{abpgraph}

\begin{figure}[h!]
\subfloat[Sender]{\label{fig:in}
\abpsender{}
}
\subfloat[Receiver]{\label{fig:in}
\abpreceiver{}
}
\caption{Program graphs of sender and receiver in the ABP protocol.}
\label{abpgraph}
\end{figure}


\subsubsection{Formal Definition of Channel Systems}
\label{CS}
A channel system CS over a set of processes P and channels Ch is a tuple $\langle$S,$s_0$,A,C,M,$\delta$$\rangle$, where 
\begin{itemize}
\item[]
S is a finite set of control states,
\item[]
$s_0$ is an initial control state,
\item[]
A is a finite set of actions,
\item[]
Ch is a finite set of channels,
\item[]
M is a finite set of messages,
\item[]
$\delta$ is a finite set of transitions, each of which is a triple of the form $\langle s_1,op,s_2\rangle$, where $s_1$ and $s_2$ are control states, and op is a label of one of the forms

\begin{itemize}
\item
c!m, where c $\in$ Ch and m $\in$ M
\item
c?m, where c $\in$ Ch and m $\in$ M
\item
a $\in$ A.
\end{itemize}
\end{itemize}

The finite-state control part of CS is an ordinary labeled transition system with states S, initial state $s_0$ and transitions $\delta$. The channel part is represented by the set Ch of channels, which may contain a string of messages in M. The set A denotes the set of observable interactions with the environment, whereas $\delta$ may either perform an action from A, or and unobservable action, where

\begin{itemize}
\item[]
$\langle s_1, c!m, s_2\rangle$ represents a change of state from $s_1$ to $s_2$ while appending the message m to the tail of channel c
\item[]
$\langle s_1, c?m, s_2\rangle$ represents a change of state from $s_1$ to $s_2$ while removing the message m to the head of channel c
\end{itemize}

\e{Example.} The alternating bit protocol can be described by a channel system CS = $\langle S,i,A,Ch,M,\delta\rangle$ such that S = {(s,r)} with \e{s} $\in$ $\{s_1,s_2,s_3,s_4\}$, \e{r} $\in$ $\{r_1,r_2,r_3,r_4\}$, \e{i} is the initial state ($s_1,r_1$), \e{A} = \{Snd, Rcv\}, \e{Ch} = \{$Ch_M,Ch_A$\}, M = \{1,0\} and delta is the set of transitions

\begin{ttabular}
$\langle s_1, Snd, s_2\rangle$ &
$\langle s_2, ch!0, s_2\rangle$ &
$\langle s_2, ch?1, s_2\rangle$ &
$\langle s_2, ch?0, s_3\rangle$ &
$\langle s_3, Snd, s_4\rangle$ &
$\langle s_4, ch!1, s_4\rangle$&
$\langle s_4, ch?0, s_4\rangle$&
$\langle s_4, ch?1, s_1\rangle$ \\

$\langle r_1, ch!1, r_1\rangle$ &
$\langle r_1, ch?1, r_1\rangle$&
$\langle r_1, ch?0, r_2\rangle$&
$\langle r_2, Rcv, r_3\rangle$&
$\langle r_3, ch!0, r_3\rangle$&
$\langle r_3, ch?0, r_3\rangle$&
$\langle r_3, ch?1, r_4\rangle$&
$\langle r_4, Rcv, r_1\rangle$
\end{ttabular}



\subsection{Channel Transition Systems}
A \e{transition system} is an abstract machine, commonly used in model checking to describe the behaviour of a system. They are in ways similar to the notion of finite state automata, with the difference that the states and transitions in a transition system need not be finite. A state of a transition system describing a program may be determined, for example, the evaluation of all variables in the program. Transitions describe how a system in a certain state can move to another state.

\subsubsection{Formal Definition of Channel Transition Systems}
\label{CTS}
The operational behaviour of CS is defined by the inifinite-state transition system TS = (C, $\rightarrow$) where
\begin{itemize}
\item[]
   C = (S $\times$ $\xi$) is the set of its configurations, where $\xi$ is an evaluation of the set of channels C in CS
\item[]
  $\rightarrow$ $\subseteq (S \times S)$ contains the following transitions
  \begin{itemize}
    \item
      For each observable action a $\in$ A in CS
      \[
      \dfrac{s \xrightarrow{a} s'}{(S, \xi) \rightarrow (S', \xi)}
      \]
    \item
      For each transmission action $\langle s_1, ch!m, s_2 \rangle$ in CS
      \[
      \dfrac{s \xrightarrow{ch!m} s' \wedge ch \in \xi}{(S, \xi) \rightarrow (S', \xi')} \] with \[ \xi' = \xi[ch := \xi (ch) \bullet m].
      \]
    \item
      For each reception action $\langle s_1, ch?m, s_2 \rangle$ in CS
      \[
      \dfrac{s \xrightarrow{ch?m} s' \wedge \xi(ch) = m \bullet w_1..w_n}{(S, \xi) \rightarrow (S', \xi')} \] with \[ \xi' = \xi[ch:= w_1..w_n].
      \]

  \end{itemize}
\end{itemize}

At times we may use a notation for a configuration c with explicit processes and channels, so that c = $\conf{s_1,...,s_n, ch_1,...,ch_p}$, or alternatively, with explicit processes and channel evaluations, c = $\conf{s_1,...,s_n, \xi(ch_1),...,\xi(ch_p)}$.

\emph{Example.} Let $c = \conf{S,\xi}$ be a configuration of the alternating bit protocol, with the channels containing the words \e{01} and \e{10} respectively. Then c may also be denoted as $c = \conf{s,r,ch_M,ch_A}$ or as $c = \conf{s_1,s_2,01,10}$. There are finitely many control states in this system, but an infinite set of channel evaluations. A transmission transition in this system is for example $\langle\conf{s_2,r_1,01,10}$, $ch_m!1$, $\conf{s_2,r_1,011,10}\rangle$. 

\subsubsection{Reachability and Bad Configurations}
An instance of the \e{reachability problem} is defined by a channel transition system TS = (S,$\xi$), a set of initial configurations \e{I} $\subseteq$ $S^+$ and a set \e{BAD} $\subseteq$ $S^+$ of \e{bad configurations}. We assume that \e{Bad} is the upward closure $\{c$ | $ c \in B: b \sqsubseteq c\}$ of a set of a given \e{finite} set of \e{minimal bad configurations}. \todo{Is this true in my case?}

Let c denote a configuration, then c is said to be \emph{reachable} in TS, if there are configurations $c_1...c_l$ such that $c_0$ is an initial configuration of TS and for each 0 $\leq$ i < l, $\langle c_i, c_{i+1} \rangle \in \rightarrow$.

We use $\mathcal{R}$ to denote the set of reachable states. We say that the system \e{TS} is \e{safe} if there are no reachable bad configurations, i.e. $\mathcal{R} \cap Bad$ = $\emptyset$.


\todo{Add a definition of a bad configuration and put it in relation to the reachability concept}

\subsection{Traces}
\todo{Define trace, minimal trace and put it into the context of bad configurations}.


\subsection{Subwords and views}
\label{subwords}
Let $\subword$ be the subword relation, then u $\subword$ $s_1...s_n$=w iff u is an ordered subset of w. For example, if w=abc, then the set of subwords of w is {abc, ab, bc, a, b, c}.

We define the \e{views} of a configurations to be \e{v'} such that for \e{c} = $\conf{s,\xi}$, \e{v'}=$\conf{s,\xi[ ch \subword \xi(ch)] | ch \in \xi}$. 
We define the size($ch$) to be equivalent to size($\xi(ch)$), i.e. the length of the evaluation (the word) on the channel \e{ch}. We define the size of a configuration or view to equal the size of its longest channel.

\emph {Example.} Suppose \e{c} is a configuration $\conf{s_1,s_2,ab,cd}$. The configuration is of size 2 and its views are

\begin{ttabular}
$\conf{s_1,s_2,ab,cd}$ \\
$\conf{s_1,s_2,a,cd}$ &
$\conf{s_1,s_2,b,cd}$ &
$\conf{s_1,s_2,\epsilon,cd}$ \\ 
$\conf{s_1,s_2,a,c}$ &
$\conf{s_1,s_2,b,c}$ &
$\conf{s_1,s_2,\epsilon,c}$ \\
$\conf{s_1,s_2,a,d}$ &
$\conf{s_1,s_2,b,d}$ &
$\conf{s_1,s_2,\epsilon,d}$ \\
$\conf{s_1,s_2,\epsilon,\epsilon}$ \\
\end{ttabular}


\subsection{View abstraction}
The abstraction function $\alpha_k: C\rightarrow 2^{C_k}$\todo{?} maps a configuration c into the set V of views of size up to \e{k} , such that for each v $\in$ V, $\{v\sqsubseteq c\}$. 

The concretization function $\gamma_k: 2^{C_k} \rightarrow 2^C$\todo{?} returns, given a set of views V, the set of configurations that can be reconstructed from the views in V, in other words, $\gamma_c(V) = \{c \in C$ | $\alpha_k(c) \subseteq V$\}

For a set \e{V}, we define the \e{post-image} of \e{V}, \e{post(V)} = \{$c'$ | \e{c} $\rightarrow$ \e{c'} $\wedge$ \e{c} $\in$ \e{V}\}. The \e{abstract post-image} of a set \e{V} $\in$ $C_k$ is defined as $Apost_k$(\e{V}) = $\alpha_k(post(\gamma_k(V)))$ In general, $\gamma_k$ is an infinite set of states. We show (\ref{proof}) that we only need to consider those configurations, whose sizes are up to k+1, i.e. a finite set of configurations. We define $\gamma_k^l(V)$ := $\gamma_k(V) \cap C_l$ for some $l\geq 0$. The intuitive meaning of $\gamma_k^l(V)$ is the set of $l$-size configurations for which all views of length at most $k$ are in $V$.

\subsubsection{Note on concretizations}
Suppose we want to determine whether \e{c} $\in$ $\gamma_k(V)$ given a configuration \e{c} and a set \e{V}. This would require that all views \e{v} $\in$ $\alpha_k(c)$ are in \e{V}. Consider a view \e{v} = $\conf{S, \xi(ch)=w}$ with size($w$) = $k$; if \e{v} $\in$ \e{V} then necessarily, any \e{v'} = $\conf{S, \xi(ch)=w'}$ with \e{w'} $\sqsubseteq$ \e{w} $\in$ \e{V}. Consequently, it is sufficient to assure that all configurations \e{c} = $\conf{S,\xi(ch_i)=w_i}$ are in \e{V}, with

\begin{itemize}
\item
size($w_i$)=$k$ if size($\xi(ch)$) $\geq$ $k$
\item
size($w_i$)= size($\xi(ch)$) if size($\xi(ch)$) < $k$.
\end{itemize}

\e{Example.} Suppose we want to determine whether \e{c} = $\conf{S, abc, def}$ is an element of $\gamma_2$. It is then sufficient to check that $\conf{S,ab,de}$, $\conf{S,ab,ef}$, $\conf{S,bc ,de}$ and $\conf{S,bc,ef}$ are in \e{V}.


\section{Problem Formulation}
In \ref{parosh}, the authors propose a verification method for \e{parameterized systems} and show that how they can, under certain assumption, be verified for an unbounded number of processes. This master thesis project is based on the results of this work, and aims to adapt this method for systems with unbounded channels. This is done by formally defining a model for systems working on channels, and proving that such systems have a \e{small system property} that allows us to verify their correctness by only looking at finite-state representations of the system.

The goal is further to implement the proposed verification method. For such an implementation to be useful, several extensions are proposed in order to expand the context in which the system can be used. Also, a specification language is defined with which the user can easily model problems. This type of verification generally has a high demand of computational resources, thus the efficiency of the implementation is of great importance. The method is used to model several well-known protocols, in order to compare its efficiency against other verification methods and establish its correctness.


\subsection{Proofs}
\label{proof}
The property that allows us to verify infinite-state problems, is that transitions have \e{small preconditions}.

\begin{lemma}
\label{lemma1}
For any $k\in\mathbb{N}$, and $X\subseteq C_k$, $\alpha_k(post(\gamma_k(X)))$ $\cup$ $X$ = $\alpha_k(post(\gamma_k^{k+1}(X)))$ $\cup$ $X$.
\end{lemma}

We will show that for any configuration \e{c} $\in$ $\gamma_k(V)$ of size $m > k + 1$ such that there is a \e{c'} induced by a transmission rule \e{r} $\in$ $\rightarrow$ from \e{c}, then for each \e{v'} $\in$ $\alpha_k(c')$, the following holds: There is a configuration \e{d} $\in$ $\gamma_k(V)$ of size at most \e{k}+1 with a transition \e{d} $\xrightarrow{r}$ \e{d'} with \e{v'} $\in$ $\alpha_k(d')$. 

\subsubsection{Transmission rules}
\label{proofTransmission}
First we note, that for any configuration \e{c} $\in$ \e{V}, any view \e{v'} $\in$ $\alpha_k(c)$ is also a valid configuration \e{c'} $\in$ $\gamma_k(V)$, since $\alpha_k(v')$ $\subseteq$ $\alpha_k(c)$ and thus \e{v'} $\in$ $\gamma_k^{k+1}(\alpha_k(c))$. Also note that if from \e{c} a transition \e{r} can be fired, then this transition can also be fired from any configuration \e{c'} = \e{v'}, as transmission rules are guarded only by the states of the channel system and not by channel evaluations.

A transmission rule changes the evaluation of at most one channel \e{ch} $\in$ \e{c}, and (possibly) the state of the channel system, thus we need only reason about the evaluations of a single channel \e{ch}.
Let \e{c} = $\conf{S, w}$ $\xrightarrow{ch!w_{m+1}}$ $\conf{S', w \bullet m}$ = \e{c'}.

The views of \e{c'} of size up to k are either of the type 1) $\conf{S', w' \sqsubset w}$, with size($w'$) $\leq$ $k$ (i.e. not including the newly transmitted message) or 2) of the form $\conf{S', w' \bullet m | w' \sqsubseteq w}$ with size($w'$) < $k$.

For any view of type 1, there exists a configuration of size $k$, \e{d} = $\conf{S, w'}$ $\in$ $\alpha_k{c}$ and the transition $r$ can be taken, resulting in \e{d'} = $\conf{S, w'\bullet m}$ of size $k$+1. The view \e{v'} $\in$ $\alpha_k{w'}$.

For any view of type 2, there exists a configuration of size $k-1$, \e{d} = $\conf{S, w'}$ and the transition $r$ can be taken resulting in \e{d'} = $\conf{S, w'\bullet m}$ = $v'$.

\e{Example}. Assume a system with two processes and a single channel. Let \e{c} = $\conf{1,2,abc}$ $\rightarrow{ch!d}$ $\conf{2,2,abcd}$. Assume that $\e{c}$ $\in$ $\gamma_2(V)$, then $\alpha_2(c)$ $\in$ \e{V}, i.e. 
$\conf{1,2,a}$, $\conf{1,2,b}$, $\conf{1,2,c}$, $\conf{1,2,ab}$, $\conf{1,2,bc}$, $\conf{1,2,ac}$ are in in V and also in $\gamma_k(V)$.

$\alpha_2(c')$ = \{$\conf{2,2,a}$, $\conf{2,2,b}$, $\conf{2,2,c}$, $\conf{2,2,d}$, $\conf{2,2,ab}$, $\conf{2,2,bc}$, $\conf{2,2,cd}$, $\conf{2,2,ac}$, $\conf{2,2,ad}$, $\conf{2,2,bd} $ \}. Consider a view with the newly transmitted message, $\conf{2,2,cd}$, it can by created by $\conf{1,2,c}$ $\rightarrow{ch!d}$ $\conf{2,2,cd}$. Considering instead a view without the transmitted message, $\conf{1,2,ab}$, it can be created by $\conf{1,2,ab}$ $\rightarrow$ $\conf{2,2,abd}$ for which $\conf{2,2,ab}$ is a view.

\subsubsection{Reception rules}
\label{proofreception}
As opposed to the transmission rules, reception rules also rely on the state of the channel in order to be fired, but only the state of that channel need be considered. Consider \e{c} = $\conf{S, m\bullet w}$ $\xrightarrow{ch?m}$ $\conf{S, w}$. For any view \e{v'} $\in$ $\alpha_k(c')$ with the word \e{w'} of size at most \e{k} on the channel, there exists a configuration of size at most \e{k+1}, \e{d} = $\conf{S, m\bullet w'}$ $\in$ $\alpha_k(c)$ such that \e{d} $\xrightarrow{ch?m}$ \e{d'} = \e{v'}.
\todo{Why is that so?}

\subsubsection{Actions}
Actions can in this context be seen as equivalent to a reception rule, reading the empty symbol $\epsilon$ on some channel. The proof then follows directly from \ref{proofreception}.



\section{Extensions}
\label{extensions}
There are several ways the channel system and the channel transition system in \ref{CS} and \ref{CTS} could be extended, in order to cope with various application scenarios. For example, we may want to model a protocol working on LIFO buffers, rather than the FIFO buffers described above. Another context may be that of a protocol communicating over an unreliable channel, introducing the possibility of \e{message loss} on the channels. In this section, we shall create models for both of these scenarios. Doing this requires us to first adapt the channel system model and the corresponding channel transition system by adding appropriate transition rules and action, and second to prove that \ref{lemma1} holds for these models.

\subsection{LIFO Channels}
\paragraph{Stack Channel System}
\label{StackCS}
In order to model a LIFO channel or a \e{stack}, we need only modify one of the transitions of the system described in \ref{CS} in such a way that transmissions and reception append and delete messages on the same end of the channels, For simplicity, we only restate this part of the channel system below. The finite-state control part of CS is an ordinary labeled transition system with states S, initial state $s_0$ and transitions $\delta$. The channel part is represented by the set Ch of channels, which may contain a string of messages in M. A set A denotes the set of observable interactions with the environment, whereas $\delta$ may either perform an action from A, or and unobservable action, where

\begin{itemize}
\item[]
$\langle s_1, c!m, s_2\rangle$ represents a change of state from $s_1$ to $s_2$ while appending the message m to the head of channel c
\item[]
$\langle s_1, c?m, s_2\rangle$ represents a change of state from $s_1$ to $s_2$ while removing the message m to the head of channel c
\end{itemize}

\paragraph{Stack Channel Transmision System}
In order to describe the transition system induced by a ChannelCS, we need only modify the transition rules of \ref{CTS} so that they reflect the changes made in \ref{StackCS}. Leaving the rest of the model unchanged, this results in $\rightarrow$ $\subseteq (S \times S)$ containing the additional transitions
\begin{itemize}
    \item
      For each observable action a $\in$ A in CS
      \[
      \dfrac{s \xrightarrow{a} s'}{(S, \xi) \rightarrow (S', \xi)}
      \]
    \item
      For each transmission action $\langle s_1, ch!m, s_2 \rangle$ in CS
      \[
      \dfrac{s \xrightarrow{ch!m} s' \wedge ch \in \xi}{(S, \xi) \rightarrow (S', \xi')} \] with \[ \xi' = \xi[ch := m \bullet \xi (ch)].
      \]
    \item
      For each reception action $\langle s_1, ch?m, s_2 \rangle$ in CS
      \[
      \dfrac{s \xrightarrow{ch?m} s' \wedge \xi(ch) = m \bullet w_1..w_n}{(S, \xi) \rightarrow (S', \xi')} \] with \[ \xi' = \xi[ch:= w_1..w_n].
      \]
  \end{itemize}

\paragraph{Proof of Lemma 1}
Only the part of the proof of lemma 1 regarding transmission rules is affected by the changes made to this system. Such a proof follows in a straightforward manner from the proof \ref{proofTransmission}, by considering configurations of the form  $\conf{S, m\bullet w'}$ rather than $\conf{S, w'\bullet m}$.

\subsection{Lossy Channels}
A lossy channel system is a system similar to \ref{CS}, with the difference that the messages on channels may be lost. In practice, data loss may appear in several contexts, e.g. data corruption, inconsistencies on weak memory models or message loss during data transmission over a network.

\paragraph{Lossy Channel Systems}
A lossy channel system is described by \ref{CS} with an additional transition $\langle s, ch*, s\rangle$ that removes a message from a channel without changing the control state.

\paragraph{Lossy Channel Transition Systems}
A lossy channel transition system TS is described by \ref{CTS} with an additional transition rule
      \[
      \dfrac{s \xrightarrow{ch*} s \wedge \xi(ch) = w_1..w_{k-1}\bullet m \bullet w_{k+1}..w_n}{(S, \xi) \rightarrow (S, \xi')} \] with \[ \xi' = \xi[ch:= w_1..w_n].
      \]

\paragraph{Proof of Lemma 1}
Trivial?  Hope so.

\subsection{Channel Systems with Synchronization}
It is not uncommon that distributed programs rely on \e{synchronization} in their program behaviour. With synchronization, we mean that two or more programs take a joint step, i.e. a transition cannot be fired unless the programs fire it together. This is particularly common in parallel programs, which may perform independent calculations but occasionally need to rendezvous. As we shall see, \ref{abpobserver}, synchronization can also be used as a modelling technique even if the program being modelled does not synchronize.

\paragraph{Channel Systems with Synchronization}
The channel system described in \ref{CS} already has the mechanisms needed in order to model synchronizing programs.\todo{Correct?}

\paragraph{Channel Transition System with Synchronization}
We modify the transition

      \[
      \dfrac{s \xrightarrow{a} s'}{(S, \xi) \rightarrow (S', \xi)}
      \]

in such a way, that an action with a label \e{l} can only be taken, if every program with that action take the action simultaneously. This corresponds to the transition

\todo{I cannot model this without talking about specific programs, which is not possible in the current definition of a channel system. I could add specific synchronization actions to that model?}


\subsection{Alternating Bit Protocol Revised}
The alternating bit protocol is a protocol designed to be resistent to message loss, therefore it is reasonable to model it using a lossy model. Furthermore, the transition system induced by the program graphs \e{abpgraph} does not provide an intuitive way to describe a set \e{Bad} of bad states. This can easily be overcome by introducing an \e{observer} program, which synchronizes with the sender and receiver.

\abpobserver

The observer synchronizes with the sender over transitions with the label \e{Snd} and with the receiver over \e{Rcv}. If either the sender performs two transmissions (with different sequence numbers) without the receiver having received in between, or if the receiver receives two messages without the sender having transmitted in between, the observer would reach its accepting state $o_3$. This state can therefore be considered to be a minimal bad state, and any configurations describing a system with the observer in its bad state is a bad configuration.

\section{Verification method}
\label{alg1}
Having proven lemma \ref{lemma1}, we use the results to ... \todo{complete this}

\begin{algorithm}
  \caption{Verification algorithm}\label{euclid}
  \begin{algorithmic}[1]
      \For{\texttt{$r\not=0$}}
        \If {$\mathcal{R}_k$ $\cap$ $Bad$ $\neq$ $\emptyset$} 
        \State return Unsafe
        \EndIf
        \State V := $\mu X.\alpha_k(I)$ $\cup$ $Apost_k(X)$
        \If {$\gamma_k(V)$ $\cap$ $Bad$ = $\emptyset$} 
        \State return Safe     
        \EndIf
      \EndFor
\end{algorithmic}
\end{algorithm}





\section{Implementation}

\subsection{Finding Minimal Traces}
When running the tool, if a bad configuration is found (or if the user just wants to know for some reason) we want to produce a trace, leading up to the bad state. Preferably, this would be a minimal trace that leads to the bad configuration.
 
The proposed verification method generatates a finite set of reachable configurations (henceforth referred to as \e{nodes} in this context), but it does not record the available transitions, henceforth \e{edges} between the nodes. It is possible for each node \e{n'} to save all nodes \e{n} with an edge resulting in \e{n'} and so build up a graph. For such a graph, there are efficient algorithms that find the shortest path between to nodes, or even the shortest paths between any two nodes. Unfortunately this approach has potential efficiency issues: there may be many more available edges in the system than nodes, which may strain memory. Also, each time a new transition to a node is found, that node needs to be updated with the new information.
 
We show that the verification method as proposed in \ref{alg1} will generate new nodes in such a way, that if a node $n_{i}$ created in the \e{i}:th iteration is reached by a node $n_{i-1}$ over an edge $e_{i-1}$, the shortest path from the initial node $n_0$ will necessarily be a path $e_1...e_i$.
 
\e{Proof.} This is proven using an induction proof. We hypothesize that, if at the point of creation of $n_i$, choosing the parent node $n_{i-1}$ from which an edge $e_{i-1}$ can be taken $n_i$, the path $e_1..e_{i}$ will be the shortest path to $n_i$ and has length \e{i}. Note that the node $n_{i-1}$ must have been created in the previous iteration; had it been created earlier, the edge $e_i$ could have been taken in a previous iteration, and so $n_{i+1}$ would already be a node in the tree.
 
The base case is that for any node reachable from $n_0$ over any edge $e_0$, $e_0$ will be the shortest path and has length 1. This is trivially true.
 
Now suppose a node $n_{i+1}$ is reachable over an edge $e_i$ from a node $n_i$, and the node $n_{i+1}$ is not yet in the system. The induction hypothesis states that the path $e_1...e_i$ is the shortest path leading up to $n_i$. If $e_0..e_{i-1}e_i$ would not be the shortest path to $n_{i+1}$, there would be a path $e'_0..e'_{k-1}$ to another node $n_k$ with k < i from which $n_{i+1}$ can be reached. But any such node would have been created in the \e{k}:th iteration of the algorithm, which would contradict the fact that the node $n_{i+1}$ was not already in the system.
 
Having shown this, we need only record the information of a single parent of a node, in order to build up a tree from which the shortest path from $n_0$ to any node in the system can efficiently be found.

