\section{Naive Implementation of the Verification Method}
\label{naive}
In this chapter, we show how the algorithm in \ref{verificationalgorithm} could be implemented. The implementation is na誰ve, in that it strictly follows the mathematical concepts without taking the time to perform the calculations into account. Finally, we identify the main drawbacks of this na誰ve implementation and suggest some improvements on the algorithm.

\paragraph{Definitions}
In this chapter, we use the terms \e{concretizations} and \e{potential concretizations}. The reason for this is that no straigh-forward way to was found to go from a set of views to a corresponding set of concretizations, i.e. from a set $X_k$ find $\gamma(X_k)$. Instead, in order to find the concretizations, we generate all of the \e{potential} concretizations, such that a potential configuration is any configuration $con$, $size(v) < size(con) \leq k+1$ for some $v \in X_k$, i.e. any extension of a view already in the set with any symbol in the alphabet.

Maintaining a set $X_k$ of views of size at most $k$, a view $v\in X_k$ may be extended with a symbol on one or more of its channels, yielding the potential concretization $con$. We say that $con$ is \e{accepted}, if all the views of $con$ of size up top $k$ are in $X_k$, otherwise, $con$ is \e{refuted}. Note that a concretization may be refuted in one iteration but accepted in another, as the set of views $X_k$ may grow to include a larger set of views. We say that the a concretization $con$ is \e{reached} from the view $v$.

As an example, if $\conf{s, a, b} \in X_k$, then $\conf{s, a, bb}$ is a potential configuration. We say that $con$ is \e{accepted}, if $\alpha_k(con) \subseteq X_k$, otherwise, $con$ is \e{refuted}. Note that a concretization may be refuted in one iteration but accepted in another, as the set of views $X_k$ may grow to include a larger set of views. We say that the an accepted concretization $con$ is \e{reached} from the view $v$.

\subsection{Na誰ve implementation}
\label{apost}
A na誰ve way of implementing algorithm \ref{verificationalgorithm} would be to implement it in a way that corresponds exactly to the mathematical notations. This is possible to do, as most programming languages have built-in data structures that support set operations. We maintain a \e{set} $X_k = \{v | size(v) < k\}$ of views such that all views are uniquely stored in $X_k$, and we will assume that an alphabet, the set $Bad$ of bad states and a set of transitions \e{R} describing the transitions are known. The the implementation of the algorithm performs three tasks:

\begin{enumerate}
\item
Compute the set $\mathcal{R}_k$ of reachable configurations of size up to k, used on line 2 of algorithm \ref{verificationalgorithm}. In section \ref{part2} we show how to compute the set $\mathcal{R}_k$.

\item
Compute the set $V_k$ of configurations, i.e. the fixpoint of $X_k$. We know that such a fixpoint exists, due to the Knaster-Tarski theorem\cite{tarski}, and we compute it by iteratively applying the Abstract post-image of $X_k$. This is used on line 3 of algorithm \ref{verificationalgorithm}. The procedure is explained in section \ref{part1}.
\todo[inline]{In your e-mail you suggested referencing the kleen iteration. I could do this, but I have not mentioned it in the previous chapters, which I probably should do if I choose to reference it here. I believe it should be enough to state that a fix-point must be reached, as the set $V_k$ is finite. Otherwise, where should I mention it and in what depth?}
%Now to compute it, we need only to compute the concretisation of size size at most k+1 and then the fixpoint can be compute  using the Kleen iteration...
\item
We need to be able to compute the intersection of a set of configurations with the set $Bad$ of bad configurations. This is done both on line 2 and 3 of algorithm \ref{verificationalgorithm}. The procedure is explained below in section \ref{part3}.
\end{enumerate}

\subsubsection{Abstract Post-image}
\label{part1}
In order to compute the abstract post-image, the concretization function, the post-image and the abstraction functions below are applied in order:

\begin{enumerate}
\item
\textbf{Concretization function}:

Generate a set $\gamma_k^{k+1}$: For each view \e{v} $\in$ $X_k$, generate all potential concretizations \e{con}. For any such potential concretization \e{con}, remove those for which $\alpha_k(con)$ $\not\subseteq$ $X_k$. This results in the set of accepted potential concretizations.

\item
\textbf{Post-image}:

Generate the set $post(\gamma(X_k))$: For every configuration $c$ $\in$ $\gamma_k^{k+1}(X_k)$, $\rho$ $\in$ $\delta$, compute $\rho(c)$, i.e. the post-image of the concretization $c$.

\item
\textbf{Abstraction Function}:
\todo[inline]{Why is abstraction function unclear?}
Generate $Apost(X_k)$: For each \e{c} $\in$ $\rho(\gamma_k^k+1(X_k))$, compute the set $V'$ of views of size at most \e{k}. If \e{V'} $\cup$ $X_k$ = $X_k$, a fixpoint has been reached, otherwise repeat the process with $X_k$ := $X_k$ $\cup$ $V'$.
\end{enumerate}

\subsubsection{Reachable configurations, $\mathcal{R}_k$}
\label{part2}
On line 2 of algorithm \ref{verificationalgorithm}, the set of all reachable configurations of size up to $k$, $\mathcal{R}_k$, is required. Computing the set $\mathcal{R}_k$ can be done in a multitude of ways. A simple way is to iteratively calculate the post-image of each configuration in the set, as follows:

we maintain a set $R$, initially containing only the initial configuration$\conf{s^0, \xi^0}$. Then iteratively, for each $c \in R$, and for each $\rho \in \delta$ compute $\rho(c)$. $\rho(c) \in R$ if  $size(\rho(c)) \leq k$.

If $size(\rho(c)) = l > k$, there is a buffer overflow. Here we assume that such an overflow is handled by removing the last symbols of the word, i.e., if $c = \conf{s, w_1, w_2,..., w_i, ...,  w_m}$ with $w_i = a_1a_2...a_ka_{k+1}...a_l$ is reached, the configuration $c' = \conf{s, w_1, w_2, ..., w_i', ..., w_n}$ with $w_i' = a_1a_2...a_k$ is added to the set $R$.

\subsubsection{Checking for bad configurations}
\label{part3}
A bad configuration is any configuration which is in a \e{state} considered to be bad, independent of the channel evaluation. Thus, the set $Bad$ can be expressed as a minimal state of configurations, such that and bad configuration $b \in Bad$ is of the format $b = (s, \xi^0)$. Checking for bad configurations can be done by for for each configuration $c = (s, \xi)$ check if $c' = (s, \xi^0) \in Bad$.

\todo[inline]{Actually, this paragraph would need an explanation on how to find a bad trace leading to the bad configuration. I've written about that in chapter 6 already, and I suggest I leave it there for now.}

\paragraph{Pseudo-code}.
The pseudo-code implementing algorithm \ref{verificationalgorithm} can be seen in listing \ref{naive}. 

\begin{algorithm}
  \caption{Pseudo-code for algorithm \ref{alg1}.}
	\label{naive}
  \begin{algorithmic}[1]
    \State \textbf{Verifier (V, Rules, Bad):}
	\State $X_k$ := \{($s_0$, $\xi^0$)\}
	\State V' := $\emptyset$
	\State $R_k$ := \{($s_0, \xi^0$)\}
	\State R' := $\emptyset$
    \For{\texttt{$True$}}
	\State \% Calculation of $R_k$
	\State\textbf{while} ($R_k$ $\neq$ $R_k$ $\cup$ $R'$)
	\State \hspace{10pt} $R_k$ := $R_k$ $\cup$ $R'$
        \State \hspace{10pt} $R'$ := $post(R')$
        \If {$\mathcal{R}_k$ $\cap$ $Bad$ $\neq$ $\emptyset$}
        \State return Unsafe
        \EndIf 
	\State 
	\State \% Fix-point calculation
	\State\textbf{while} ($X_k$ $\neq$ $X_k$ $\cup$ $V'$)
	\State \hspace{10pt} $X_k$ := $X_k$ $\cup$ $V'$
        \State \hspace{10pt} $V'$ := $\alpha_k(post(\gamma_k^{k+1}(V)))$
        \If {$X_k$ $\cap$ $Bad$ = $\emptyset$} \todo[inline]{Question was "is it X or the concretization of X?". It is X, but it would not matter. A bad configuration is only bad because of its state, and any concretization in X would have the same state as some view already in X, as new states are only reached when the post-image is computed.}
        \State return Safe
        \EndIf
        \State k := k+1
      \EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Improving the Algorithm}
Evaluating the procedure described above, we find that the bottle-neck lies within the concretization function:
\todo[inline]{Comment was "can you give an example". The following is a reasoning why this would happen. Giving a concrete example would likely not make things more clear.}
 In order to compute $\gamma(X_k)$, all potential concretizations of the set $X_k$ are comuted by creating all extensions of the views $v \in X_k$, i.e. all potential configurations $con$ created from $v$ such that at least one word of the channel evaluation is of a larger size than in $v$. For each of these potential concretizations, $\alpha(con)$ is created in order to determine whether the concretization should be accepted or refuted.

Although this method is correct, there is significant overlap of concretizations being considered as it is often possible to create the same concretization \e{con} in several ways. For example, if $X_k$ contains the views $v_1$ = $\conf{s, \epsilon, ab}$ and $v_2$ = $\conf{s, ab, \epsilon}$, then the concretization $\conf{s, ab,ab}$ can be reached from both $v_1$ and $v_2$, by adding the symbols $ab$ to the first and second channel of $v_1$ and $v_2$ respectively.

A rough worst-case estimate of the number of potential concretizations created in each iteration in the above algorithm can be given. If we let $s$ denote the size of the alphabet, $t$ the number of channels and $n$ = $size(X_k)$ in a certain iteration we can create upper limits on the number of potential concretizations that are considered. A potential concretization is created for each combination of symbols with which a word of the channel evaluation can be extended, in the worst case $s^k$ combinations (the number of words of size $k$ that can be created from an alphabet of $s$ symbols). Also, each combination of such extensions on the $t$ different channels is created, resulting in $(s^k)^t$ potential configurations for a single view. As this must be done for each of the $n$ existing views, the result is a worst case of $O(n*(s^k)^t)$ potential concretizations that may be considered in a single iteration.

This leads to the conclusion that decreasing the number of potential concretizations considered may result in a faster algorithm. Below we show that this can be achieved by redefining the subword relation, originally defined in section \ref{words}. Using the new definition of the subword relation, algorithm \ref{verificationalgorithm} will still reach the fixpoint $V_k$, i.e. reachable concretizations will be found.

\paragraph{Simplified Subword Relation}
Using the fact that the verification algorithm is recursive, ending first when no new configurations can be found, we can re-define the subword relation such that only a subset of the subwords are included, but still guaranteeing that when used within algorithm \ref{verificationalgorithm}, all views are eventually found.

\subparagraph{Definition.} 
\label{newsubword}
We redefine the subword relation in section \ref{words}, such that a word $w' \models w = a_1a_2\ldots a_n$ if $w' = a_ia_{i+1}\ldots a_j$ for some $0 \leq i \leq n$, $0 \leq j \leq n$, where $\models$ denotes the new subword relation. As an example, if \e{w}=$abc$, then the set of subwords of \e{w} is \e{abc, ab, bc, a, b, c, $\varepsilon$}. Note that if $w' \models w \implies w' \subword w$, but $w' \subword w \centernot\implies w' \models w$. 

\begin{lemma}
If the set $X$ denote a fixpoint of algorithm \ref{verificationalgorithm} reached using the definition of the subword relation as defined in section \ref{words}, and the set $Y$ denote a fixpoint of the algorithm using the above definition of the subword relation, then $X=Y$.
\end{lemma}

\begin{proof}
Consider a word $w' = a_ia_{i+1}\ldots a_ja_la_{l+1}\ldots a_m$, $i \geq 0$, $l > j+1$, then $w' \subword w = a_1a_2\ldots a_n$, but $w' \centernot\models w$.
Let $v'$ be a view with the word $w'$ on one of its channels, then if $v' \in V_k$, i.e. $v'$ should be accepted, then the set $V_k$ of views necessarily also contains views $v'_1$, $v'_2$, \ldots with the words $a_ia_{i+1}\ldots a_j$, $a_ia_{i+1}\ldots a_ja_l$, $a_ia_{i+1}\ldots a_ja_la_{l+1}\ldots a_{m-1}$ on the channel respectively.

The view $v'_1$ with the word $a_ia_{i+1}\ldots a_j \models w$ would be created, as it is an unbroken sequence of messages. Then the view $v'_2$ with the word $a_ia_{i+1}\ldots a_ja_l$ is a potential concretization of $v'_1$, which would be accepted. The view $v'_3$ with the word $a_ia_{i+1}\ldots a_ja_la_{l+1}$ on the channel is in turn a potential concretization of $v'_2$, which would again be accepted. Continuing in this way, the view with the word $w'$ on the channel will eventually be created and accepted.
\end{proof}


\paragraph{Reducing the number of concretizations.}
We can show that when creating the potential concretizations of a view $v$, we need not generate the full set of potential concretizations and instead limit the algorithm to generate all those potential concretizations $con$ reachable from $v$ by the extension of a single channel evaluation with a single symbol.
%Given a configuration \e{c}, with channel evaluations $\xi$ such that $size(c) \leq k$, a potential concretization of \e{c} is any configuration \e{con} such that \e{con} can be created from \e{c} by appending at least one message to at least one of the channels of \e{c}.
%This is a large number of potential configurations, for which it would be desireable to consider only a subset, while still ensuring that all valid concretizations are eventually found.

Consider a potential concretization $con = (s, w'_1, w'_2, \ldots, w'_n)$ reached from the view $v = (s, w_1, w_2, \ldots, w_n)$ such that $w'_i \models w_i$ for each $i \leq n$. If the generation of potential concretizations would be limited such that only one word $w_i$ would be extended in each iteration, the concretization $con$ would still eventually be reached, as the views $v \subword \conf{s, w'_1, w_2, w_3 \ldots, w_n} \subword \conf{s, w'_1, w'_2, w_3, \ldots, w_n} \subword \ldots \subword \conf{s, w'_1, w'_2, \ldots, w'_{n-1}, w_n} \subword con$.

% If such a potential concretization is accepted, any view $v'$ where $n'<n$ channels have been modified in a similar way will also be a valid concretization, since $v' \subword v$. Therefore, \e{con} will eventually be considered also when only one channel is extended in each iteration, but $n$ iterations are required in order to reach it.
Using this approach, the number of potential concretizations considered each iteration is reduced significantly. Using the same notation as above, we let $s$ denote the size of the alphabet, $t$ the number of channels and $n$ = $size(X_k)$ in a certain iteration, the number of potential concretizations is reduced such that each of the $n$ views is extended with at most 1 symbol on each of its channels, resulting in a worst case of $O(n*s*t)$ potential concretizations. Although the number of potential configurations generated each iteration is reduced, the number of iterations required in order to create the potential configurations increases.

\paragraph{Reducing the number of views.}
When generating potential configurations as explained above, the task of accepting or refuting the concretizations becomes more effective.  
Suppose we want to determine whether a potential concretization \e{con} reached from $v$ should be accepted or refuted. We know that $con$ has been genereated from $v$ by adding a single symbol $m$ on one of the channels of $v$, whereas the state of $con$ and $v$ are the same, as is the content of the other channels.

\begin{lemma}
Say $con = (s, w_1, \ldots, w_n)$ was created by extending some view $v = (s, w'_1, \ldots, w'_n) \in X_k$ by adding a single message to a single channel such that $w_p = w'_p \bullet m$ for some $p\leq m$ and $w_i = w'_i$ for all $i \neq p$. Then, if $w_p = m_1, m_2 \ldots m$, it suffices to check whether there is a view $v' = (s, w_1, \ldots, w_n) \in X_k$ such that $w_p = m_2\ldots m$, in order to determine if $con$ can be accepted or not.
\end{lemma}

\begin{proof}
Using the simplified subword relation, the views of $con$ either contain the new symbol $m$ in the word $w_p$, or they do not. Any view of $con$ not containing the symbol $m$ in $w_p$ is also a view of $v$, and must exist in $X_k$. Any view containing $m$ in the word $w_p$ does so with $m$ as its last symbol, i.e. there is a prefix of size at most $k$ followed by the message $m$. Any such word is a subword of the largest possible view containing the message $m$, i.e. $m_2\ldots m$.

\end{proof}


\subparagraph{Example.} Suppose we want to determine whether the potential concretization \e{con} = $\conf{s, abc, de}$ created from the view $v = (s, ab, de)$ $\in X_k$. It is then sufficient to check that $v'$ = $\conf{s,bc,de}$ is an element of $X_k$, as any other view is either a subword of $v$ and is already in $X_k$, or a subword of $v'$, and therefore in $X_k$ if $v' \in X_k$.

\newpage
\section{Improved Implementation}
The last chapter showed how the calculation of the set $V_k$ of configurations could be made more effective by simplifying the concretization and abstraction function, leading to a lower computational complexity in each iteration for the cost of an increased number of iterations. In this chapter, we further improve the implementation of the algorithm, by storing a partitioned version of the set $X \subseteq V_k$ of configurations and the set $R$ of transitions.

The algorithm implementation in chapter \ref{naive} uses a \e{set} to store configurations, therefore the time to insert and perform a lookup in the set increases with its size, which can often be expected to grow large, due to often unavoidable state-space explotion. Using an ordered set, inserting an element to the set and finding an element in the set is done in \e{O(log n)} where \e{n} denotes the number of elements in the set.

We will show that partitioning the set of configurations into a combined hashmap and set data structure, the time required to perform any set operation can be significantly reduced. Additionally, by structuring the set $R$ of transitions in a similar manner, the time to calculate the post-image of configurations can be decreased as well.

\subsection{Control-State Partitioning}
Consider a configuration $c \in X \subseteq V_k$. By definition the views of $c$ = $\conf{s, \xi}$ has views of type $\conf{s,\xi'}$, i.e., the configuration and its views have the same control-state as $c$. The time required for inserting the views of $c$ to the set $X$, i.e performing the set operation $\alpha(c) \cup X$ or conversely, checking whether the views of $c \subseteq X$ (in order to accept or refute $c$), depends on the size of the set $X$, in which all configurations are uniquely stored. By partitioning the set $X$ into several disjoint sets, such that each set contains only configurations with the same control-state, these tasks can be performed more effectively.

Let $S$ denote the finite set of control-states and $W$ denote the finite set channel configurations. We define a \e{hashmap} $X_m$, indexed by control-states $s \in S$ and storing sets of channel evaluations $\xi \in W$. Then $X_m$ can be viewed as a functions $X_m : s \in S -> W_s \subseteq W$, such that $\bigcup\limits_{s\in S} X_m(s)$ = $X$.

Similarly, consider the application of a transition $\rho = (s, op, s') \in \delta$ to a configuration $c$. If $state(c) \neq s$, $\rho(c)$ is not applicable on $c$. Having partitioned the configurations into a hashmap as explained above, we can easily find all the configurations on which $\rho$ may be applicable, simply by finding $X_m(s)$.

We define $\delta_m : s -> R \subseteq \delta$ to be a hashmap in a similar manner, such that any $\rho \in R$ is of the form $(s, op, s')$. Again, $\bigcup\limits_{s\in S} \delta_m(s)$ = $\delta$.

We can now redefine the concretization function, post-image and abstraction function defined in section \ref{apost}, as follow:

\todo[inline]{The next paragraph is left un-updated on purpose. In order to make this as clear as possible, I want the phrasing to be as close as possible to the one in the last chapter as possible, and that chapter is still under construction.}
\paragraph{Abstract Post-image.}
In order to compute the abstract post-image, the concretization function, the post-image and the abstraction functions below are applied in order:

\begin{enumerate}
\item
\textbf{Concretization function}:

Generate a set $\gamma_k^{k+1}$: For each view \e{v} $\in$ $X_k$, generate all potential concretizations \e{con}. For any such potential concretization \e{con}, remove those for which $\alpha_k(con)$ $\not\subseteq$ $X_k$. This results in the set of accepted potential concretizations.

\item
\textbf{Post-image}:

Generate the set $post(\gamma(X_k))$: For every configuration $c$ $\in$ $\gamma^{k+1}_k(X_k)$, $\rho$ $\in$ $\delta$, compute $\rho(c)$, i.e. the post-image of the concretization $c$.

\item
\textbf{Abstraction Function}:
Generate $Apost(X_k)$: For each \e{c} $\in$ $\rho(\gamma_k^k+1(X_k))$, compute the set $V'$ of views of size at most \e{k}. If \e{V'} $\cup$ $X_k$ = $X_k$, a fixpoint has been reached, otherwise repeat the process with $X_k$ := $X_k$ $\cup$ $V'$.
\end{enumerate}


\paragraph{example}
Figure \ref{applyrule} illustrates the calculation of the post-image using the partitioned data-types. In order to clarify the procedure, the hashmaps are illustrated in the form of trees such that each leaf corresponds to a set $W_s$ of channel evaluations or $\delta_s$ of transitions, and the path from the root node to the leaf corresponds to the control-state.

\begin{figure}
\includegraphics[width=400pt] {bilder/applyrule.png}
\caption{This picture shows the process of applying rules. For each control-state, each rules with that state are applied on each configuration with the same state. This will generate new configurations, not necessarily with the same control-state. For clarity.  tree structure is used to illustrate a hashmap.}
\label{applyrule}
\end{figure}



\subsection{Advantages of Partitioning}
Having partitioned the set $X$ of configurations into a hashmap of $X_k$ of sets, all set operations are made more effective. In a best-case scenario, there would be approximately the same number of configurations for every state in the system. The complexity of checking whether $\alpha_k(c)$ $\in$ $V$ is reduced to $O(log (n/|S|))$ where $S$ is the set of control-states in the system. The complexity of inserting an element is naturally the same.

Additionally, the complexity of calculating the set of post-images of a set $\gamma_k^{k+1}(X)$ is reduced, as we need not perform the calculation of each each $\rho = (s, op, s') \in \delta$ on each $c \in \gamma_k^{k+1}(X)$, but only on the subset of configurations for which $state(c) = s$.


\begin{algorithm}
  \caption{The verification algorithm from section \ref{alg1} in somewhat higher detail. This version includes }\label{euclid}
  \begin{algorithmic}[1]
    \State \textbf{Gamma (V, Seen):}
    \State \hspace{6 mm} con' := concretizations (V)
    \State \hspace{6 mm} con  := c | c $\in$ con $\land$ c $\notin$ Seen
    \\
    \State \textbf{Step (Con, Rules):}
    \For {state $\in$ nodes(V)}
    \State \hspace{6 mm} S := r(c) | $\forall$ c $\in$ con(state) $\land$ $\forall$ r $\in$ Rules(state)
    \EndFor
    \\
    \State \textbf{Alpha (V, S):}
    \State \hspace{6 mm} V:= V $\cup$ views(C)
    \\
    \State \textbf{Verifier (V, Rules, Bad):}
    \For{\texttt{$True$}}
        \If {$\mathcal{R}_k$ $\cap$ $Bad$ $\neq$ $\emptyset$}
        \State return Unsafe
        \EndIf
        \State V := $\mu Alpha(Step(Gamma(V)))$
        \If {$\gamma_k(V)$ $\cap$ $Bad$ = $\emptyset$}
        \State return Safe
        \EndIf
        \State k := k+1
      \EndFor
\end{algorithmic}
\end{algorithm}



\newpage
\section{Final Implementation}
An efficient algorithm in this context is largely an algorithm that avoids performing unnecessary calculations. This can either be done by avoiding to create unnecessary configurations such as was done with the rule hashmap above or by avoiding re-calculating previously calculated results. The algorithm as described above reproduces its steps each iteration; if a configuration \e{c} can be extended to a concretization \e{con} at any point in the verification process, then \e{con} can and will be created in every following iteration. This includes checking whether \e{$\alpha_k(con) \subset V$}, applying a set of rules to the configuration and then adding all the views of the resulting configurations to the set. Each time the calculations are performed, the result will be duplicates and no new views are added to the system.

We solve this by maintaining another hashmap, \e{seen}, of configurations in parallell, containing exactly those concretization that have been accepted. If a configuration \e{c} can be extended to the concretization \e{con}, then we first check if \e{con} is an element of seen. If it is, we discard \e{con}, otherwise we add \e{con} to \e{seen} and also to the set of concretizations to be evaluated in this iteration.

Yet another source of repetition is the fact that there are multiple ways to create the same channel evaluations. Therefore, after a rule has been applied to a concretization, it may result in a configuration already in the set. Instead of performing the costly $\alpha$-calculation, we first check if the newly created configuration is not in fact a duplicate by checking if it is already in \e{V}. If so, the configuration can again be discarded.

The final algorithm amounts to the following pseudo-code representation:

\subsection{Reachability Analysis}
An important step of the verification process which has yet to be covered is that of performing a reachability analaysis, in order to find bad states, if any such states are reachable and find a minimal bad trace leading up to the bad state. See sections \ref{bad} and \ref{traces} for formal definitions of bad states and traces respectively.

Below, a simple technique of finding a minimal bad trace is presented, accompanied with a proof that the trace is in fact minimal.

\subparagraph{Finding Minimal Traces}
When running the verification, if a bad state is found we want to produce a trace leading up to the bad state. Preferably, this would be a minimal trace that leads to the bad state.

The proposed verification method generates a finite set of reachable states (nodes in this context), but it does not record the available transitions between the nodes (i.e. the edges). It is possible to for each node \e{n} to save all nodes \e{n} from which an edge to \e{n'} exists, and thus build the complete reachability graph of the problem. There exists efficient algorithms to solve such a problem, e.g. \e{dijkstra shortest path}, or even the shortest path between any two nodes, e.g. flow-network techniques. Although these algorithms are efficient, building the complete reachability graph would be costly in terms of memory space, as the number of edges may be much larger than the number of states.


We show that due to the method of iteratively constructing the graph, nodes are created in such a way, that if a node $n_{i}$ created in the \e{i}:th iteration is reached by a node $n_{i-1}$ over an edge $e_{i-1}$, the shortest path from the initial node $n_0$ will necessarily be a path $e_1...e_{i-1}$.

\e{Proof.} This is proven using an induction proof. We hypothesize that, if at the point of creation of $n_i$, choosing the parent node $n_{i-1}$ from which an edge $e_{i-1}$ can be taken $n_i$, the path $e_1..e_{i}$ will be the shortest path to $n_i$ and has length \e{i}. Note that the node $n_{i-1}$ must have been created in the previous iteration; had it been created earlier, the edge $e_i$ could have been taken in a previous iteration, and so $n_{i+1}$ would already be a node in the tree.

The base case is that for any node reachable from $n_0$ over any edge $e_0$, $e_0$ will be the shortest path and has length 1. This is trivially true.

Now suppose a node $n_{i+1}$ is reachable over an edge $e_i$ from a node $n_i$, and the node $n_{i+1}$ is not yet in the system. The induction hypothesis states that the path $e_1...e_i$ is the shortest path leading up to $n_i$. If $e_0..e_{i-1}e_i$ would not be the shortest path to $n_{i+1}$, there would be a path $e'_0..e'_{k-1}$ to another node $n_k$ with k < i from which $n_{i+1}$ can be reached. But any such node would have been created in the \e{k}:th iteration of the algorithm, which would contradict the fact that the node $n_{i+1}$ was not already in the system.

Having shown this, we need only record the information of a single parent of a node, in order to build up a tree from which the shortest path from $n_0$ to any node in the system can efficiently be found.


\subsection{Specification Language}
\label{speclang}
In order for the verification algorithm to be easily used, a specification language is needed in which algorithms can be formally defined. We expect such a specification language to

\begin{itemize}
\item
be expressive enough to express all algorithms that are in the scope of the verification program
\item
be independent of the internal representations of the verifier and to demand as little knowledge of the actual verification process as possible from the user
\item
to be as clear as possible in order to ensure that the model at hand in fact corresponds to the actual algorithm, the way it was intended
\end{itemize}

The specification language used is an adaption of previous works in \cite{mpass}. The language simply uses XML to describe an algorithm. There are minor differences between the specification language used here, with respect to that used in by the MPass verification algorithm, and correspond to different expressivness of the models. More specifically, the MPass verifier allows for joint reception and transmission transitions, i.e. a single transition may read a message from a channel and write a message to a channel at the same time. On the other hand, it does not allow for synchronized transitions, this has therefore been added to the language.

\subsubsection{Example}
We return yet again to the Alternating Bit Protocol to examplify the specification language. Bearing in mind the formal definition in section \ref{CS}, a model needs a set of messages, channels, actions and transitions.

We begin by specifying a new protocol, it's name and that it is operating on FIFO buffers. We then specify a set of messages, a set of channels and a set of actions in a similar manner.

\lstset{language=XML}
\begin{lstlisting}[frame=single]
<protocol name="Alternating Bit Protocol" medium="FIFO>
<messages>
      <message>ack0</message>
      <message>ack1</message>
      <message>mesg0</message>
      <message>mesg1</message>
</messages>

<channels>
      <channel>c1</channel>
      <channel>c2</channel>
</channels>

<actions>
      <action>Rcv</action>
      <action>Snd</action>
</actions>
\end{lstlisting}

We then continue by defining the different processes in the system, i.e. the sender, receiver and observer. Such a process is defined by its states and its transitions. In the specification language, we differentiate between transitions over an action, and transitions that modify a channel. The latter type is called a \e{rule} in the specification language, in order to comply with the names used in the MPass specification language. This should not be confused with the word as used earlier in this section.

\begin{lstlisting}[frame=single]
  <role name="SENDER">
    <states>
      <state type="initial">Q0</state>
      <state>Q1</state>
      <state>Q2</state>
      <state>Q3</state>
    </states>
    <action>
        <current_state>Q0</current_state>
        <type>Snd</type>
        <next_state>Q1</next_state>
    </action>
    <rule>
        <current_state>Q1</current_state>
        <send_message>mesg0</send_message>
        <next_state>Q1</next_state>
        <channel>c1</channel>
    </rule>
\end{lstlisting}

Last, we specify that the processes \texttt{SENDER} and \texttt{OBSERVER}, and  \texttt{RECEIVER} and \texttt{OBSERVER} are to be synchronized over the actions \texttt{Snd} and \texttt{Rcv} respectively.

\begin{lstlisting}[frame=single]
<synchronize>
        <first_role>SENDER</first_role>
        <second_role>OBSERVER</second_role>
        <action>Snd</action>
</synchronize>
<synchronize>
        <first_role>RECEIVER</first_role>
        <second_role>OBSERVER</second_role>
        <action>Rcv</action>
</synchronize>
\end{lstlisting}
